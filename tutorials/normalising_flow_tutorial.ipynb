{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75e6b81",
   "metadata": {},
   "source": [
    "# A Conditional Normalising Flow Tutorial\n",
    "---\n",
    "This notebook extends the previous Higgs pT regression example by learning a **full probability distribution** for the Higgs $p_{T}$ using a **RealNVP normalising flow**. \n",
    "\n",
    "As opposed to predicting a single point estimate, we train via **maximum likelihood** so that we can sample or evaluate a density at arbitrary $p_{T}$ values, conditioned on the jet features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ffd2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reuse existing setup for data prep, etc. TODO: find way to nicely link back?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2dcea",
   "metadata": {},
   "source": [
    "## RealNVP Building Blocks\n",
    "We define a simple 1D `RealNVP` transform, made **conditional** on the jet features (treated as our `context`).  \n",
    "\n",
    "For multiple coupling layers, we stack them to build a more flexible flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ca9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RealNVP Coupling Layer for 1D target\n",
    "# condition on D-dim context [batch, context_dim=flattened_jet_features]\n",
    "\n",
    "# In 1D, the coupling transform is basically: \n",
    "#   y = x * exp(s(context)) + t(context)\n",
    "\n",
    "# We need ensure to invertibility, so we flip input half-splits or sign flips, etc.\n",
    "# where a flip is just a sign flip on the Jacobian determinant [J.det = -J.det].\n",
    "\n",
    "class ConditionalRealNVPCoupling(nn.Module):\n",
    "    def __init__(self, context_dim, hidden_dim=64) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simple MLP that outputs scale and shift as functions of the `context`.\n",
    "        \n",
    "        # For the 1D target, scale & shift are just scalars per event.\n",
    "        \n",
    "        # the scale network\n",
    "        self.net_s = nn.Sequential(\n",
    "            nn.Linear(context_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            # outputs scale\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # the shift network\n",
    "        self.net_t = nn.Sequential(\n",
    "            nn.Linear(context_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            # outputs translation\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context, reverse=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape [batch, 1], the target pT dimension.\n",
    "            context: shape [batch, context_dim], the jet features.\n",
    "            reverse: if True, invert the flow.\n",
    "        Returns:\n",
    "            y: shape [batch, 1], transformed variable.\n",
    "            log_abs_det: the log of absolute determinant of the Jacobian.\n",
    "        \"\"\"\n",
    "        s = self.net_s(context)  # [batch, 1] -> scale\n",
    "        t = self.net_t(context)  # [batch, 1] -> shift\n",
    "        \n",
    "        if not reverse:\n",
    "            # forward transform: \n",
    "            # y = x * exp(s) + t\n",
    "            y = x * torch.exp(s) + t\n",
    "            \n",
    "            # log det = sum over dims, but here dims=1 => just s\n",
    "            log_abs_det = s.squeeze(dim=-1)  # shape [batch]\n",
    "        else:\n",
    "            # inverse transform: \n",
    "            # x = (y - t) * exp(-s)\n",
    "            y_ = x\n",
    "            x = (y_ - t) * torch.exp(-s)\n",
    "            y = x\n",
    "            \n",
    "            # log det = -s for the inverse\n",
    "            # log det = sum over dims, but here dims=1 => just s\n",
    "            log_abs_det = -s.squeeze(dim=-1)\n",
    "        \n",
    "        # return the transformed variable and the log determinant of the Jacobian\n",
    "        return y, log_abs_det\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7be95",
   "metadata": {},
   "source": [
    "## Stacking Multiple Coupling Layers\n",
    "We can combine multiple coupling layers (with sign flips or permutations) for a richer transformation. \n",
    "\n",
    "Below is a minimal container that holds **N** coupling layers and composes them. \n",
    "\n",
    "In 1D, we often just flip the sign or add an identity pass each time to ensure that each layer operates differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ecabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVPFlow(nn.Module):\n",
    "    def __init__(self, context_dim, n_coupling_layers=4, hidden_dim=64) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            ConditionalRealNVPCoupling(context_dim, hidden_dim=hidden_dim)\n",
    "            for _ in range(n_coupling_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        \"\"\" Forward pass: transforms base -> data space.\n",
    "            We'll treat x as base-samples (z).\n",
    "        \"\"\"\n",
    "        logdet_sum = 0.0\n",
    "        y = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            y, logdet = layer(y, context, reverse=False)\n",
    "            logdet_sum += logdet\n",
    "        return y, logdet_sum\n",
    "\n",
    "    def inverse(self, y, context):\n",
    "        \"\"\" Inverse pass: transforms data -> base space.\n",
    "            This is used for log-likelihood computation.\n",
    "        \"\"\"\n",
    "        logdet_sum = 0.0\n",
    "        x = y\n",
    "        # inverse in reverse order:\n",
    "        for i, layer in reversed(list(enumerate(self.layers))):\n",
    "            x, logdet = layer(x, context, reverse=True)\n",
    "            logdet_sum += logdet\n",
    "        return x, logdet_sum\n",
    "\n",
    "    def log_prob(self, y, context):\n",
    "        \"\"\"Compute log p(y|context).  \n",
    "        \n",
    "        We transform y -> x in base, and add log p_x(x) + log|det J|.\n",
    "        Base distribution is standard Normal(0,1) in 1D.\n",
    "        \"\"\"\n",
    "        # inverse transform y -> x\n",
    "        x, logdet = self.inverse(y, context)  \n",
    "        # base log prob ~ -0.5 * x^2 - 0.5*log(2pi)\n",
    "        log_p_x = -0.5*(x**2) - 0.5*np.log(2*np.pi)\n",
    "        log_p_x = log_p_x.squeeze(dim=-1)  # shape [batch]\n",
    "        return log_p_x + logdet\n",
    "\n",
    "    def sample(self, context, n_samples=1):\n",
    "        \"\"\"Draw samples from p(y|context).  In 1D, x ~ N(0,1), then forward.\n",
    "        \"\"\"\n",
    "        # context shape: [batch, context_dim]. We'll sample for each item.\n",
    "        batch_size = context.shape[0]\n",
    "        z = torch.randn(batch_size*n_samples, 1)  # random normal samples for base space\n",
    "        \n",
    "        # repeat context to match that shape\n",
    "        # or do it in a loop if you prefer.  We'll tile the context.\n",
    "        repeated_context = context.repeat_interleave(n_samples, dim=0)\n",
    "        \n",
    "        # forward transform z -> y\n",
    "        y, _ = self.forward(z, repeated_context)\n",
    "        \n",
    "        # return the samples\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442455f1",
   "metadata": {},
   "source": [
    "## Training the Flow\n",
    "Instead of MSE we used in the previous regression tutorial, we use the **negative log-likelihood** loss:\n",
    "$$\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^N \\log p(\\text{Higgs}\\ p_T^{(i)} \\mid \\text{jet features}^{(i)})$$\n",
    "\n",
    "Below is a typical loop: you pass \n",
    "`(y, context)` → compute `flow.log_prob(y, context)` → maximise that log-prob (or minimise the negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169af4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow config\n",
    "\n",
    "# you may recognise this as the same input dim for the previous regression tutorial\n",
    "context_dim = (N_JETS * N_FEATURES) \n",
    "\n",
    "# build the flow\n",
    "flow = RealNVPFlow(context_dim=context_dim, n_coupling_layers=4, hidden_dim=64)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "flow.to(device)\n",
    "\n",
    "optimiser = optim.AdamW(flow.parameters(), lr=1e-3)\n",
    "n_epochs = 10  # or more\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    flow.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader_reg:  # same loader as before, but now we'll interpret `batch['target']` as y, and `batch['jets']` as context.\n",
    "        context = batch['jets'].to(device)  # shape [B, N_JETS, N_FEATURES]\n",
    "        context = context.reshape(context.size(0), -1)  # flatten\n",
    "        y = batch['target'].to(device).unsqueeze(-1)  # shape [B, 1]\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # log_prob has shape [batch], so we take mean of negative for loss.\n",
    "        log_p = flow.log_prob(y, context)\n",
    "        loss = -log_p.mean()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        total_loss += loss.item() * len(y)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader_reg.dataset)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    # TODO: add a progress bar and improve logging here!\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}]  NLL: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55563625",
   "metadata": {},
   "source": [
    "## Inference & Sampling\n",
    "With a trained flow, you can now:\n",
    "\n",
    "1. **Compute** $p(\\hat p_T \\mid X)$ for any $\\hat p_T$. \n",
    "    - This is the probability of the Higgs pT given the jet features.\n",
    "  \n",
    "2. **Sample** from the distribution to see the variety of plausible pT outcomes.\n",
    "    - This is the distribution of the Higgs pT given the jet features.\n",
    "\n",
    "3. **Evaluate** the log-likelihood of the test set.\n",
    "    - This is the log-likelihood of the Higgs pT given the jet features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c954b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate log p on test set + produce samples\n",
    "flow.eval()\n",
    "all_logp = []\n",
    "all_pt_truth = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_reg:\n",
    "        context = batch['jets'].to(device)\n",
    "        context = context.reshape(context.size(0), -1)\n",
    "        y = batch['target'].to(device).unsqueeze(-1)\n",
    "        log_p = flow.log_prob(y, context)\n",
    "        all_logp.append(log_p.cpu().numpy())\n",
    "        all_pt_truth.append(y.cpu().numpy())\n",
    "all_logp = np.concatenate(all_logp, axis=0)\n",
    "all_pt_truth = np.concatenate(all_pt_truth, axis=0)\n",
    "\n",
    "print(f\"Average log probability on test set: {all_logp.mean():.4f}\")\n",
    "\n",
    "# As an example, we can generate samples for 1 batch of test context.\n",
    "sample_batch = next(iter(test_loader_reg))\n",
    "context_sample = sample_batch['jets'][:10].to(device)\n",
    "context_sample = context_sample.reshape(context_sample.size(0), -1)\n",
    "\n",
    "# Now, we can draw samples from the flow!\n",
    "samples = flow.sample(context_sample, n_samples=100)  # shape [10*100, 1]\n",
    "\n",
    "print(\"Samples shape:\", samples.shape)\n",
    "print(\"Example pT samples:\", samples[:10].detach().cpu().squeeze().numpy())\n",
    "\n",
    "# Cool right!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose 'flow' is trained, and we have a test batch of size B, sample from the flow over a set number of test batches.\n",
    "\n",
    "# for batch in test_loader_reg:\n",
    "#     all_truth.append(batch[\"target\"].cpu().numpy())\n",
    "# truth_np = np.concatenate(all_truth, axis=0)\n",
    "\n",
    "batches = [next(iter(test_loader_reg)) for _ in range(10)]  # Use 2 batches\n",
    "context = torch.cat([batch[\"jets\"].to(device) for batch in batches]).reshape(-1, context.size(1))  # [B*2, context_dim]\n",
    "true_scaled = torch.cat([batch[\"target\"].to(device).unsqueeze(-1) for batch in batches])  # [B*2, 1] scaled pT\n",
    "\n",
    "# -----> SAMPLE from the flow\n",
    "n_samples_per_event = 1000  # (flows per event)\n",
    "samples_scaled = flow.sample(context, n_samples=n_samples_per_event)  \n",
    "# shape = [B*n_samples_per_event, 1]\n",
    "\n",
    "# -----> Inverse-scale both \"samples_scaled\" and \"true_scaled\" so they're back in [MeV or GeV]\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # useful if we are on a GPU\n",
    "    samples_np = samples_scaled.cpu().numpy()\n",
    "    true_np    = true_scaled.cpu().numpy()\n",
    "\n",
    "    # Invert with 'target_scaler' (the one used for pT, not the feature scaler!)\n",
    "    samples_unscaled = target_scaler.inverse_transform(samples_np)  # shape [B*n_samples, 1]\n",
    "    truth_unscaled   = target_scaler.inverse_transform(true_np)     # shape [B, 1]\n",
    "\n",
    "# conert to GeV\n",
    "samples_GeV = samples_unscaled / 1000.0\n",
    "truth_GeV   = truth_unscaled   / 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc37f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(samples_GeV.flatten(), bins=50, alpha=0.5, density=True, label=\"Flow Samples (GeV)\")\n",
    "plt.hist(truth_GeV.flatten(),   bins=50, alpha=0.5, density=True, label=\"Truth pT (GeV)\")\n",
    "plt.xlabel(\"Higgs pT [GeV]\")\n",
    "plt.ylabel(\"Normalized Counts\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467eb085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the flow cpature spread of pT?\n",
    "\n",
    "print(f\"Truth Variance: {np.var(truth_GeV):.2f}, Sample Variance: {np.var(samples_GeV):.2f}\")\n",
    "\n",
    "# plot kde of the samples and the truth idea\n",
    "sns.kdeplot(samples_GeV.flatten(), label=\"Flow Samples KDE\", fill=True)\n",
    "sns.kdeplot(truth_GeV.flatten(), label=\"Truth pT KDE\", fill=True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# does the flow predict reasonable quantiles?\n",
    "percentiles = [10, 25, 50, 75, 90]\n",
    "truth_quantiles = np.percentile(truth_GeV, percentiles)\n",
    "sample_quantiles = np.percentile(samples_GeV, percentiles)\n",
    "\n",
    "for p, tq, sq in zip(percentiles, truth_quantiles, sample_quantiles):\n",
    "    print(f\"{p}th percentile - Truth: {tq:.2f}, Samples: {sq:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc6f28",
   "metadata": {},
   "source": [
    "Soe things to consider\n",
    "Flow Samples May Be Overfitting to High-Density Regions, i.e. the flow learns by maximising log-likelihood, so if some bins of pT have far more training points than others, it might over-prioritsze learning those regions, leading to sharp peaks and gaps in less-represented regions. The flow is also parametric, meaning that it can only model distributions it has seen enough examples of.\n",
    "\n",
    "Next tutorial idea ---> Neural Spline Flows; allow non-linear monotonic transformations! since the flow is monotonic, it can learn the inverse transform, and hence the quantiles. REAL NVPs only allow linear transformations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
