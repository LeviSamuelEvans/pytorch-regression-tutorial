{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Regression with PyTorch in High Energy Physics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "This is a basic tutorial to get you going with using PyTorch for regression tasks in High Energy Physics.\n",
    "It is not meant to demonstrate the most optimal application by any means, but rather to provide an `walk-through` style guide.\n",
    "\n",
    "There is an additional notebook which will walk-through a more powerful method.\n",
    "\n",
    "The basic example here will operate on `ready-made` high-level features in a multivariate manor. Hence, our input feature vector will be `flat`. In the other notebook, we use a more complex method that instead operates on particle objects, e.g. jets, leptons, etc., with each object described by a vector of features. This will be referenced as a `particle set`.\n",
    "\n",
    "Furthermore, with regards to `MLOps`, there are many features we will not be adding to this example, such as `hyperparameter tuning`, `model selection`, `cross-validation`, `early stopping`, `logging`, `monitoring`, `checkpoints`, serialisation to `ONNX` and ntuple injection etc.\n",
    "\n",
    "One final note, we do not consider any `MC` weights in this example.\n",
    "\n",
    "We assume you have the correct enviroment setup to run this notebook, following the instructions in the README.md file.\n",
    "\n",
    "Okay, let's get started!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Task\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "#                  SETUP BLOCK                  #\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "\n",
    "# All the libraries we will need for this tutorial :)\n",
    "# See the README.md file for more information on how to install these libraries\n",
    "# and get your environment setup if you haven't already!\n",
    "\n",
    "# ================================================\n",
    "# --> Data manipulation:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# --> PyTorch associated libraries:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --> Scikit-learn associated libraries:\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# --> Plotting libraries:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# ================================================\n",
    "\n",
    "\n",
    "# basic style for any plots\n",
    "# ================================================\n",
    "# --> some more options are: \n",
    "#       seaborn-darkgrid, grayscale\n",
    "#       seaborn-dark, seaborn-whitegrid, \n",
    "#       seaborn-white, seaborn-ticks,\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"husl\") \n",
    "# ================================================\n",
    "\n",
    "# Path to h5 file (stored in git LFS)\n",
    "# ================================================\n",
    "ttH_path = '../data/ttH_fullSim_dev4vec_150k.h5'\n",
    "# ================================================\n",
    "\n",
    "# All global variables\n",
    "# ================================================\n",
    "PLOT_INPUTS = False                   # type: bool  \n",
    "N_JETS = 7                            # type: int   \n",
    "N_FEATURES = 5                        # type: int   \n",
    "HIDDEN_DIM = 128                      # type: int   \n",
    "REGRESSION_TARGET = 'truth_higgs_pt'  # type: str   \n",
    "TEST_SIZE = 0.2                       # type: float \n",
    "BATCH_SIZE = 64                       # type: int   \n",
    "NUM_EPOCHS = 30                       # type: int\n",
    "DEBUG = False                         # type: bool  \n",
    "LEARNING_RATE = 0.001                 # type: float \n",
    "MODEL_DIR = '../models'               # type: str\n",
    "MODEL_NAME = 'basic_model'            # type: str\n",
    "SEED = 42                             # type: int\n",
    "# ================================================\n",
    "# Extra truth-match information for Higgs b-jets\n",
    "B_LEAD_HIGGS_VALUES = [1.0]           # type: list[float] # -> Leading b from Higgs\n",
    "B_SUBLEAD_HIGGS_VALUES = [2.0]        # type: list[float] # -> Subleading b from Higgs\n",
    "B_BOTH_HIGGS_VALUES = [3.0]           # type: list[float] # -> Both leading and subleading in same jet\n",
    "# ================================================\n",
    "# NOT NEEDED FOR THIS TUTORIAL (SETTRANSFORMERMODEL)\n",
    "EMBEDDING_DIM = 128                   # type: int\n",
    "\n",
    "print(\"\\033[92m -- All global variables set and setup ran successfully! --\\033[0m\")\n",
    "\n",
    "# References:\n",
    "# -----------\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "#            INITIALISING DATA BLOCK            #\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "\n",
    "# Now, we are going to load the dataframes and combine them\n",
    "def load_data(file_path, process_name) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the data from the h5 file and return a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the h5 file.\n",
    "        process_name (str): The name of the process to load.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing the data.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        dfs = []\n",
    "        # the data is stored individually for each monte-carlo tag\n",
    "        for version in ['mc16a', 'mc16d', 'mc16e']:\n",
    "            group = f[f'IndividualFiles/combined_{process_name}_PP8_{version}_root']\n",
    "\n",
    "            # we need to decode the bytes to strings for column names in the h5 file\n",
    "            block0_columns = [name.decode('utf-8') for name in group['block0_items'][:]]\n",
    "            block1_columns = [name.decode('utf-8') for name in group['block1_items'][:]]\n",
    "            block2_columns = [name.decode('utf-8') for name in group['block2_items'][:]]\n",
    "\n",
    "            # then we need to get the associated values for each block\n",
    "            block0_values = group['block0_values'][:]\n",
    "            block1_values = group['block1_values'][:]\n",
    "            block2_values = group['block2_values'][:]\n",
    "\n",
    "            # then we need to concatenate the values for each block\n",
    "            # and create a dataframe for each version\n",
    "            df_version = pd.concat([\n",
    "                pd.DataFrame(block0_values, columns=block0_columns),\n",
    "                pd.DataFrame(block1_values, columns=block1_columns),\n",
    "                pd.DataFrame(block2_values, columns=block2_columns)\n",
    "            ], axis=1)\n",
    "\n",
    "            df_version['mc16_version'] = version\n",
    "            dfs.append(df_version)\n",
    "\n",
    "        df = pd.concat(dfs, axis=0, ignore_index=True) # we ignore the index to avoid duplicate indices!\n",
    "\n",
    "        df['target'] = df[f'{REGRESSION_TARGET}'].astype(float)\n",
    "        return df\n",
    "\n",
    "\n",
    "# call the function to load the data and assign it to a the variable df\n",
    "ttH_df = load_data(ttH_path, 'ttH')\n",
    "df = ttH_df\n",
    "\n",
    "print(\"\\033[92m -- All data loaded successfully! --\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model definition\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "#             MODEL DEFINITION BLOCK            #\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "\n",
    "# A very basic neural network with 3 layers\n",
    "class MyBasicModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic neural network with 3 layers, inherited from the `nn.Module` class.\n",
    "    ----\n",
    "    See https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n",
    "    \n",
    "    This is a basic neural network with 3 layers. \n",
    "    It is used to demonstrate the basic concepts of a neural network in PyTorch.\n",
    "    \n",
    "    In the init method, we define the actual layers of our network. For this, we use the `nn.Linear` class. \n",
    "    This class creates a linear transformation of the input.\n",
    "    \n",
    "    For example, if we have an input of size `input_dim`, then the `nn.Linear` layer will create a matrix of size \n",
    "    `input_dim x hidden_dim`. The entries of this matrix are simply the `weights` of the linear transformation, \n",
    "    which are randomly initialised.\n",
    "    \n",
    "    We then apply a non-linearity to the output of the linear transformation using the `torch.relu` function in the\n",
    "    `forward` method. The `relu` function is simply the `rectified linear unit` function, which is a very simple \n",
    "    non-linearity that returns the input if it is positive, and 0 otherwise.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "        input_dim: the dimension of the input\n",
    "        hidden_dim: the dimension of the hidden layer\n",
    "        output_dim: the dimension of the output\n",
    "    \n",
    "    Returns:\n",
    "    ----\n",
    "        x: the output of the model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim) -> None:\n",
    "        super(MyBasicModel, self).__init__()\n",
    "        #print(f\"Input dim: {input_dim}, Hidden dim: {hidden_dim}, Output dim: {output_dim}\")  # Debug print\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(f\"Input shape: {x.shape}\")     # Debug print if needed\n",
    "        \n",
    "        # flatten the input\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        # print(f\"After reshape: {x.shape}\")  # Debug print if needed\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "       # print(f\"After fc1: {x.shape}\")       # Debug print if needed\n",
    "       \n",
    "        x = torch.relu(self.fc2(x))\n",
    "        #print(f\"After fc2: {x.shape}\")       # Debug print if needed\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        #print(f\"After fc3: {x.shape}\")       # Debug print if needed\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Count the number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\"\\033[92m -- Model defined successfully! --\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "#             PREPARATION BLOCK                 #\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "\n",
    "class EventDataset(Dataset):\n",
    "    def __init__(self, df , jets_cols, target_column) -> None:\n",
    "        \"\"\"\n",
    "        Custom training dataset class\n",
    "        ----\n",
    "        This class is used to create a custom training dataset for the model\n",
    "        from a pandas dataframe. It inherits from the `torch.utils.data.Dataset` \n",
    "        class.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The dataset.\n",
    "            jets_cols (list of str): Column names for jet features.\n",
    "            target_column (str): Column name for the target label.\n",
    "            regression_target (bool): Whether this is a regression task.\n",
    "        \"\"\"\n",
    "        self.jets = df[jets_cols].values\n",
    "        self.targets = df[target_column].values\n",
    "        self.n_jets = N_JETS\n",
    "        self.n_features = N_FEATURES\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"Returns the item at index idx, i.e. the jet features and target.\n",
    "        \"\"\"\n",
    "        # Reshape the jet features to [n_jets, n_features]\n",
    "        jets_sample = torch.FloatTensor(self.jets[idx]).reshape(self.n_jets, self.n_features)\n",
    "        \n",
    "        # Convert target to tensor\n",
    "        target = torch.tensor(float(self.targets[idx]), dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "                'jets': jets_sample,\n",
    "                'target': target\n",
    "            }\n",
    "\n",
    "\n",
    "def has_both_higgs_bjets(event_row) -> bool:\n",
    "    \"\"\"\n",
    "    Check if an event has both leading and subleading b-jets from Higgs.\n",
    "    \n",
    "    Args:\n",
    "        event_row (pd.Series): A row from the dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the event has both leading and subleading b-jets from Higgs, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check all jet columns for the event\n",
    "    has_leading = False\n",
    "    has_subleading = False\n",
    "    \n",
    "    for i in range(1, N_JETS + 1):  \n",
    "        truthmatch_col = f'jet_truthmatch_{i}'\n",
    "        if truthmatch_col not in event_row:\n",
    "            continue\n",
    "            \n",
    "        value = event_row[truthmatch_col]\n",
    "        \n",
    "        # check to see if the jet has both b-quarks from the Higgs decay\n",
    "        if value in B_BOTH_HIGGS_VALUES:\n",
    "            return True\n",
    "            \n",
    "        # check to see if the jet has the leading b-quark from the Higgs decay\n",
    "        if value in B_LEAD_HIGGS_VALUES:\n",
    "            has_leading = True\n",
    "            \n",
    "        # check to see if the jet has the subleading b-quark from the Higgs decay\n",
    "        if value in B_SUBLEAD_HIGGS_VALUES:\n",
    "            has_subleading = True\n",
    "            \n",
    "    # Return True if both types are found\n",
    "    return has_leading and has_subleading\n",
    "\n",
    "\n",
    "# NOTE: using the truth information is \"cheating\", but we do it here to demonstrate the method and\n",
    "# get some level of decent performance. In reality, we would not have this information when \n",
    "# running inference on data.\n",
    "\n",
    "# define the columns for the jet features we want to use in the model training\n",
    "jets_cols = [\n",
    "    'jet_pt_1', 'jet_eta_1', 'jet_phi_1', 'jet_e_1', 'jet_truthmatch_1',\n",
    "    'jet_pt_2', 'jet_eta_2', 'jet_phi_2', 'jet_e_2', 'jet_truthmatch_2',\n",
    "    'jet_pt_3', 'jet_eta_3', 'jet_phi_3', 'jet_e_3', 'jet_truthmatch_3',\n",
    "    'jet_pt_4', 'jet_eta_4', 'jet_phi_4', 'jet_e_4', 'jet_truthmatch_4',\n",
    "    'jet_pt_5', 'jet_eta_5', 'jet_phi_5', 'jet_e_5', 'jet_truthmatch_5',\n",
    "    'jet_pt_6', 'jet_eta_6', 'jet_phi_6', 'jet_e_6', 'jet_truthmatch_6',\n",
    "    'jet_pt_7', 'jet_eta_7', 'jet_phi_7', 'jet_e_7', 'jet_truthmatch_7',\n",
    "]\n",
    "\n",
    "# get the events that have both leading and subleading b-quarks from the Higgs decay\n",
    "# some events will have only one b-quark from the Higgs decay, and so we will not use these events\n",
    "# for the training of the model.\n",
    "# Benefits: \n",
    "# - We are only training on events that have both b-quarks from the Higgs decay\n",
    "# - This should give us a more accurate model as we are not mixing events with different Higgs decay topologies\n",
    "#    and events where we 'lose' one of the b-quarks.\n",
    "# Maybe it would useful to also test the model with events that have only one b-quark from the Higgs decay too\n",
    "# but we will not do this here, perhaps an exercise for the you if you want!\n",
    "complete_higgs_events = df[df.apply(has_both_higgs_bjets, axis=1)]\n",
    "\n",
    "# Next, we want to split and scale the data\n",
    "\n",
    "# split the data into training and test sets\n",
    "# the random state is set to a fixed value to ensure the same split every time the notebook is run\n",
    "# this ensures reproducibility of the results.\n",
    "df_train, df_test = train_test_split(complete_higgs_events, test_size=TEST_SIZE, random_state=SEED, shuffle=True)\n",
    "\n",
    "# scale the data using the `MinMaxScaler` scaler\n",
    "# MinMaxScaler = (x - min(x)) / (max(x) - min(x))\n",
    "# The scaler you use will depend on the input data, so read up on this!\n",
    "scaler_vars = MinMaxScaler()\n",
    "\n",
    "# save the scaler to a file so we can use it later if needed\n",
    "with open(f'{MODEL_DIR}/scaler_vars.npz', 'wb') as f:\n",
    "    np.savez(f, scaler_vars=scaler_vars)\n",
    "\n",
    "# fit the scaler to the training data\n",
    "df_train[jets_cols] = scaler_vars.fit_transform(df_train[jets_cols])\n",
    "\n",
    "# transform the test data using the fitted scaler\n",
    "# we want to apply the same scaler to the test data as we did to the training data\n",
    "# if we did not do this, the test data would be on a different scale to the training data\n",
    "# and this would bias the model\n",
    "df_test[jets_cols] = scaler_vars.transform(df_test[jets_cols])\n",
    "\n",
    "# Clean the data and target variable \n",
    "# (NOTE: data cleaning is very good practise, and this is just a simple, rough and ready, example\n",
    "# where we only remove events with a target value < 0!)\n",
    "df_train = df_train[df_train['target'] > 0]\n",
    "df_test = df_test[df_test['target'] > 0]\n",
    "\n",
    "# We should also scale the target variable as well...\n",
    "target_scaler = MinMaxScaler()\n",
    "df_train['target'] = target_scaler.fit_transform(df_train[['target']]).flatten()\n",
    "df_test['target'] = target_scaler.transform(df_test[['target']]).flatten()\n",
    "\n",
    "# save the target scaler to a file so we can use it later if needed\n",
    "with open(f'{MODEL_DIR}/target_scaler.npz', 'wb') as f:\n",
    "    np.savez(f, target_scaler=target_scaler)\n",
    "\n",
    "print(\"\\033[92m--- Data Prepared and Scaled Successfully! ---\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "#             LOADING DATA BLOCK                #\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "\n",
    "# Create datasets for regression using our custom EventDataset class\n",
    "train_dataset_reg = EventDataset(df_train, jets_cols, target_column='target')\n",
    "test_dataset_reg = EventDataset(df_test, jets_cols, target_column='target')\n",
    "\n",
    "# Create data loaders using PyTorchs native DataLoader class\n",
    "train_loader_reg = DataLoader(train_dataset_reg, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_reg = DataLoader(test_dataset_reg, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"\\033[92m--- Event Data Loaded Successfully! ---\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The SetTransformerModel is a more complex model, and is not covered in this tutorial. \n",
    "# NO NEED TO RUN THIS CELL!\n",
    "\n",
    "class SetTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads=2, num_layers=2) -> None:\n",
    "        super(SetTransformerModel, self).__init__()\n",
    "    \n",
    "        self.embedding_dim = EMBEDDING_DIM  # Using the global EMBEDDING_DIM (128)\n",
    "        self.embedding = nn.Linear(input_dim, self.embedding_dim)\n",
    "        \n",
    "        # Make sure embedding_dim is divisible by num_heads\n",
    "        assert self.embedding_dim % num_heads == 0, f\"Embedding dimension ({self.embedding_dim}) must be divisible by number of heads ({num_heads})\"\n",
    "        \n",
    "        # Multi-head self attention layers from the `nn.TransformerEncoderLayer` class\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=self.embedding_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_dim,\n",
    "                batch_first=True\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Global pooling and final prediction layers\n",
    "        self.pooling = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, n_jets, n_features]\n",
    "        \n",
    "        # Project input if necessary\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Apply self-attention layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Global pooling (mean across jets)\n",
    "        x = torch.mean(x, dim=1)  # [batch_size, embedding_dim]\n",
    "        \n",
    "        # Final prediction\n",
    "        x = self.pooling(x)  # [batch_size, 1]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Count the number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "#               REGRESSION BLOCK                #\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "\n",
    "# Now let's try regression\n",
    "print(\"\\033[92m--- Regression Task ---\\033[0m\")\n",
    "\n",
    "\n",
    "# Define the regression model by instantiating the MyBasicModel class\n",
    "regression_model = MyBasicModel(input_dim=N_FEATURES * N_JETS, hidden_dim=HIDDEN_DIM, output_dim=1)\n",
    "\n",
    "# A more complex model is below, the SetTransformerModel. NOTE: In reality, this model will not perform better than the MyBasicModel\n",
    "# We need more training data, more compute, and a more complex training procedure to really see the benefits of this model.\n",
    "# but, I've included it here just to show you a cool model that is getting a lot of `attention`;) right now.\n",
    "\n",
    "# regression_model = SetTransformerModel(input_dim=N_FEATURES, hidden_dim=HIDDEN_DIM, num_heads=2, num_layers=2)\n",
    "\n",
    "trainable_parameters = regression_model.count_parameters\n",
    "\n",
    "print(f\"INFO: Number of trainable parameters: {trainable_parameters}\\n\")\n",
    "\n",
    "print(f\"INFO: Model architecture: {regression_model}\\n\")\n",
    "\n",
    "print(f\"INFO: Number of training events: {EventDataset.__len__(train_dataset_reg)}\\n\")\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define the optimiser (AdamW is a good default choice!)\n",
    "optimiser = torch.optim.AdamW(regression_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Define the device to train on (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device (this is really only needed if you are using a GPU)\n",
    "regression_model.to(device)\n",
    "\n",
    "# Create lists to store the trainig and test losses\n",
    "train_losses_reg = []\n",
    "test_losses_reg = []\n",
    "\n",
    "# Now can start training the model!\n",
    "print(f\"INFO: Training regression model on {device}\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    regression_model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader_reg:\n",
    "        batch_X, batch_target = batch['jets'].to(device), batch['target'].to(device) # Again, only need to do this if using a GPU\n",
    "\n",
    "        # zero the gradients \n",
    "        # this is done to avoid accumulating gradients from previous batches, so for each batch we need to `zero the gradients`\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = regression_model(batch_X)\n",
    "        \n",
    "        # squeeze the output to get a 1D tensor to match the loss computation\n",
    "        outputs = outputs.squeeze()\n",
    "        batch_target = batch_target.view(-1)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_fn(outputs, batch_target)\n",
    "\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimiser.step()\n",
    "\n",
    "        # add the loss to the total loss\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    # we do a similar process to the training loop, but we do not update the weights here\n",
    "    # we only want to evaluate the model on the test set\n",
    "    # .eval() is used to set the model to evaluation mode, this is because some layers, such as dropout if we were using it, behave differently in training and evaluation mode.\n",
    "    regression_model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    # store the predictions and actual values for evaluation\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    # we do not need to track the gradients for the test set, so we use `torch.no_grad()`\n",
    "    with torch.no_grad():\n",
    "        # iterate over the test set\n",
    "        for batch in test_loader_reg:\n",
    "            batch_X, batch_target = batch['jets'].to(device), batch['target'].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = regression_model(batch_X)\n",
    "            outputs = outputs.squeeze()\n",
    "            batch_target = batch_target.view(-1) \n",
    "\n",
    "            # compute the loss\n",
    "            test_loss += loss_fn(outputs, batch_target).item()\n",
    "            \n",
    "            # Store predictions and actual values for evaluation\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(batch_target.cpu().numpy())\n",
    "\n",
    "    # compute the average loss for the epoch\n",
    "    train_losses_reg.append(train_loss / len(train_loader_reg))\n",
    "    test_losses_reg.append(test_loss / len(test_loader_reg))\n",
    "\n",
    "    # compute the R-squared score\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "\n",
    "    print(f'INFO: Epoch [{epoch+1}/{NUM_EPOCHS}], '\n",
    "          f'Train Loss: {train_losses_reg[-1]:.4f}, '\n",
    "          f'Test Loss: {test_losses_reg[-1]:.4f}, '\n",
    "          f'R-squared Score: {r2:.4f}')\n",
    "\n",
    "print(\" Trained for \", NUM_EPOCHS, \" epochs\")\n",
    "\n",
    "# and we are done training and validating the model!\n",
    "print(\"\\033[92m--- Training Completed! ---\\033[0m\")\n",
    "\n",
    "# we can save the model as a `.pth` file (this is a binary file that contains the model's state_dict)\n",
    "# alternatively, we can save the model as a `.pt` file (this is a text file that contains the model's state_dict)\n",
    "# or as a `.pkl` file (this is a pickle file that contains the model's state_dict)\n",
    "# or even as a .npz file (this is a numpy file that contains the model's state_dict)\n",
    "torch.save(regression_model.state_dict(), f'{MODEL_DIR}/{MODEL_NAME}_regression_model.pth')\n",
    "\n",
    "print(f\"\\033[92m INFO: Model saved to {MODEL_DIR}/{MODEL_NAME}_regression_model.pth\\033[0m\")\n",
    "\n",
    "# we can convert the model to ONNX format (not covered in this tutorial)\n",
    "# this is useful for deployment and for using the model in other frameworks\n",
    "# we can convert the model to ONNX format using the `torch.onnx.export` function\n",
    "# we can then load the model from the ONNX file using the `torch.onnx.load` function\n",
    "\n",
    "\n",
    "# also useful, we can load the model from the `.pth` file, and then use it to make predictions\n",
    "# or continue training the model if desired\n",
    "# regression_model = MyBasicModel(input_dim=N_FEATURES * N_JETS, hidden_dim=HIDDEN_DIM, output_dim=1)\n",
    "# regression_model.load_state_dict(torch.load('regression_model.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "#              TRAINING CURVE PLOT              #\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "\n",
    "# we can plot the training and validation losses to spot overfitting or underfitting\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses_reg, label='Training Loss')\n",
    "plt.plot(test_losses_reg, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding model performance\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "#             MIGRATION MATRIX PLOT             #\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "\n",
    "# we can create a migration matrix to see how the model performs for different Higgs pT bins\n",
    "# this is a good way to understand the model's performance and to see if there are any biases in the model.\n",
    "\n",
    "# convert predicted and actual target values to GeV\n",
    "# we need to first inverse transform the target values to the original scale, and then divide by 1000 to convert to GeV\n",
    "actuals_gev = target_scaler.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten() / 1000\n",
    "predictions_gev = target_scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten() / 1000\n",
    "\n",
    "# Define the bins for Higgs pT (from STXS-based binning)\n",
    "bins = [0, 60, 120, 200, 300, 450, np.inf]\n",
    "bin_labels = ['[0,60)', '[60,120)', '[120,200)', '[200,300)', '[300,450)', '[450,inf)']\n",
    "\n",
    "# Assign each value to a bin\n",
    "actuals_binned = np.digitize(actuals_gev, bins) - 1\n",
    "predictions_binned = np.digitize(predictions_gev, bins) - 1\n",
    "\n",
    "# Create the migration matrix\n",
    "migration_matrix = confusion_matrix(actuals_binned, predictions_binned, \n",
    "                                   labels=range(len(bin_labels)), normalize='true', )\n",
    "\n",
    "# Flip the matrix vertically to have 0-60 at the bottom\n",
    "migration_matrix = np.flipud(migration_matrix)\n",
    "flipped_bin_labels = bin_labels.copy()\n",
    "flipped_bin_labels.reverse()\n",
    "\n",
    "# Plot the migration matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cmap = sns.color_palette(\"Blues\", as_cmap=True)\n",
    "ax = sns.heatmap(migration_matrix, \n",
    "                annot=True,                      # --> Show values in cells\n",
    "                fmt='.2f',                       # --> Format to 2 decimal places\n",
    "                cmap=cmap,                       # --> Use our custom colormap\n",
    "                xticklabels=bin_labels,          # --> X-axis labels\n",
    "                yticklabels=flipped_bin_labels,  # --> Y-axis labels (flipped)\n",
    "                vmin=0, vmax=1,                  # --> Fix color scale from 0 to 1\n",
    "                annot_kws={\"size\": 12,           # --> Larger annotation text\n",
    "                           \"weight\": \"bold\"},    # --> Bold annotation text\n",
    "                linewidths=0.5,                  # --> Add thin lines between cells\n",
    "                linecolor='white',               # --> White lines between cells\n",
    "                cbar_kws={\"shrink\": 0.8,         # --> Smaller colorbar\n",
    "                          \"label\": \"Fraction of Events\"})  # Colorbar label\n",
    "\n",
    "plt.xlabel('Predicted Higgs pT (GeV)', fontsize=14, fontweight='bold', labelpad=15)\n",
    "plt.ylabel('True Higgs pT (GeV)', fontsize=14, fontweight='bold', labelpad=15)\n",
    "plt.xticks(fontsize=12, rotation=45, ha='right')\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "for _, spine in ax.spines.items():\n",
    "    spine.set_visible(True)\n",
    "    spine.set_linewidth(2)\n",
    "    spine.set_color('black')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics about the binning\n",
    "for i, label in enumerate(bin_labels):\n",
    "    true_count = np.sum(actuals_binned == i)\n",
    "    pred_count = np.sum(predictions_binned == i)\n",
    "    print(f\"Bin {label} GeV: {true_count} true events, {pred_count} predicted events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the prediction to the truth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higgs pT distribution comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(predictions_gev, bins=500, alpha=0.3, density=True)\n",
    "plt.hist(actuals_gev, bins=500, alpha=0.3, density=True)\n",
    "plt.xlabel('Higgs pT (GeV)')\n",
    "plt.xlim(0, 500)\n",
    "plt.ylabel('Normalised Events')\n",
    "plt.title('Higgs pT Distribution')\n",
    "plt.legend(['Predictions', 'Actuals'])\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of predictions vs actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(actuals_gev, predictions_gev, alpha=0.3)\n",
    "plt.plot([min(actuals_gev), max(actuals_gev)], [min(actuals_gev), max(actuals_gev)], 'r--')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals plot\n",
    "# this is a distribution of the prediction errors :--> (predicted - actual)\n",
    "plt.figure(figsize=(12, 6))\n",
    "residuals = np.array(predictions_gev) - np.array(actuals_gev)\n",
    "plt.hist(residuals, bins=50, density=True)\n",
    "plt.xlabel('Prediction Error [GeV]')\n",
    "plt.ylabel('Normalised Events')\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"INFO: Regression target statistics (GeV): min={actuals_gev.min():.2f}, max={actuals_gev.max():.2f}, mean={actuals_gev.mean():.2f}\")\n",
    "print(f\"INFO: Regression prediction statistics (GeV): min={predictions_gev.min():.2f}, max={predictions_gev.max():.2f}, mean={predictions_gev.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print some statistics about the predictions\n",
    "mean_error = np.mean(residuals)\n",
    "mean_abs_error = np.mean(np.abs(residuals))\n",
    "rmse = np.sqrt(np.mean(np.square(residuals)))\n",
    "\n",
    "print(f\"INFO: Mean Error: {mean_error:.2f}\")\n",
    "print(f\"INFO: Mean Absolute Error: {mean_abs_error:.2f}\")\n",
    "print(f\"INFO: Root Mean Square Error: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix I - Information on the Dataset (h5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
